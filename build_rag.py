import json
import os
import sys
import chromadb
from llama_index.core import Document, StorageContext, Settings
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.node_parser import SentenceSplitter

# --- ì„¤ì • ---
BASE_DIR = "./data"
INPUT_JSON_PATH = os.path.join(BASE_DIR, "bokjiro_rag_final.json")
DB_PATH = "./chroma_db"
COLLECTION_NAME = "welfare_policy"

# ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (OLLAMA_NUM_PARALLEL ê°’ê³¼ ë§ì¶°ì£¼ì„¸ìš”)
NUM_WORKERS = 4 

# --- ì„ë² ë”© ëª¨ë¸ ì„¤ì • ---
embed_model = OllamaEmbedding(
    model_name="mxbai-embed-large",
    base_url="http://host.docker.internal:11434", 
    ollama_additional_kwargs={"mirostat": 0},
    # ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë ¤ì„œ í•œ ë²ˆì— ë§ì´ ì²˜ë¦¬ (GPU ë©”ëª¨ë¦¬ í™œìš©)
    embed_batch_size=32 
)

Settings.embed_model = embed_model
Settings.llm = None 

def load_documents_from_json():
    if not os.path.exists(INPUT_JSON_PATH):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {INPUT_JSON_PATH}")
        sys.exit(1)

    with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:
        data_list = json.load(f)

    documents = []
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘... ({len(data_list)}ê±´)")

    for item in data_list:
        text_content = item.get('rag_full_text', '')
        if not text_content.strip(): continue 

        metadata = {
            "service_id": item.get('service_id', 'unknown'),
            "service_name": item.get('service_name', 'ì œëª© ì—†ìŒ'),
            "department": item.get('department', ''),
            "url": item.get('url', '')
        }

        doc = Document(
            text=text_content,
            metadata=metadata,
            excluded_embed_metadata_keys=["url", "service_id", "service_name"] 
        )
        documents.append(doc)
    
    return documents

def build_index_parallel():
    documents = load_documents_from_json()
    if not documents: return

    print(f"ğŸ’¾ ChromaDB ì—°ê²° ì¤‘... ({DB_PATH})")
    db_client = chromadb.PersistentClient(path=DB_PATH)
    chroma_collection = db_client.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

    # --- [í•µì‹¬] ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ---
    print(f"ğŸš€ ë³‘ë ¬ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ì‹œì‘ (Workers: {NUM_WORKERS})")
    
    pipeline = IngestionPipeline(
        transformations=[
            # 1. ë¬¸ì„œë¥¼ ì²­í¬(Chunk)ë¡œ ìë¥´ê¸° (ì˜ë¯¸ ë‹¨ìœ„ 512 í† í°)
            SentenceSplitter(chunk_size=512, chunk_overlap=50),
            # 2. ì„ë² ë”© (ë²¡í„° ë³€í™˜)
            embed_model,
        ],
        vector_store=vector_store, # ë³€í™˜ëœ ë°ì´í„°ë¥¼ ë°”ë¡œ DBì— ì €ì¥
    )

    # ë³‘ë ¬ ì‹¤í–‰
    # run í•¨ìˆ˜ê°€ ë¬¸ì„œë¥¼ ìª¼ê°œê³  -> ì„ë² ë”©í•˜ê³  -> DBì— ë„£ëŠ” ê³¼ì •ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    pipeline.run(documents=documents, num_workers=NUM_WORKERS)
    
    print("\nğŸ‰ [ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ] ëª¨ë“  ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    try:
        # ê¸°ì¡´ DBê°€ ìˆë‹¤ë©´ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ì•Œë¦¼
        if os.path.exists(DB_PATH):
            print("â„¹ï¸  ê¸°ì¡´ DB í´ë”ì— ì¶”ê°€(Upsert)í•˜ê±°ë‚˜ ë®ì–´ì”ë‹ˆë‹¤.")
            
        build_index_parallel()
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")